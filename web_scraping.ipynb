{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1e11237f-cdeb-4555-a541-55bc9545cd57",
      "cell_type": "markdown",
      "source": "### Web Scraping\n",
      "metadata": {}
    },
    {
      "id": "c0baaa29-2011-4863-aa8d-24b3cf641f79",
      "cell_type": "markdown",
      "source": "## Import all necessary libraries",
      "metadata": {}
    },
    {
      "id": "2b7d014b-4231-4f09-9850-4b4abc46f9e7",
      "cell_type": "code",
      "source": "import re\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "da7d37d9-a19d-417d-a3f6-a1398bf533ea",
      "cell_type": "markdown",
      "source": "## Scraping bioinformatics jobs",
      "metadata": {}
    },
    {
      "id": "b56ae472-853a-4076-805a-024e5996451d",
      "cell_type": "markdown",
      "source": "### Get the page",
      "metadata": {}
    },
    {
      "id": "582592fa-0f72-4d84-a1ed-464624d9781e",
      "cell_type": "code",
      "source": "LENGTH = 10 # number of jobs to retrieve\nJOB_URL = 'https://www.bioinformatics.org/jobs/?group_id=101&summaries=1&length=%s' % (LENGTH-1)\n\n# get the HTML of the page with requests.get()\nr = requests.get(JOB_URL)\n\n# now pass the page content to bs4\nsoup = BeautifulSoup(r.content)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "acaaaf20-b7b6-47e7-93f5-18e69053ba9f",
      "cell_type": "markdown",
      "source": "### Get the elements of interest\n\nLooking at the HTML of the web page, job opportunities are in  _tables_. The job opportunities are in tables where the text \"Opportunity\" is part of the content.\n\nGet all the tables, and then retain only those with \"Opportunity\" in the content, storing them in a list.",
      "metadata": {}
    },
    {
      "id": "441c1c3d",
      "cell_type": "code",
      "source": "# get all the tables\ntables = soup.find_all('table')\n\n# the jobs are prefixed by the text \"Opportunity\".\n# Check this with the attribute \"text\" available for each table.\nfor table in tables:\n    print(table.text)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ada67c1f-664d-4a80-adcd-e9aec95e129f",
      "cell_type": "code",
      "source": "# store the interesting tables in a list\nentries = list()\nfor table in tables:\n    if 'Opportunity' in table.text:\n        entries.append(table)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "de4f8498-d3f4-451b-a389-895a0569f17d",
      "cell_type": "code",
      "source": "# remove the first two items of the list and retain the remaining items\nentries = entries[2:]",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5b1dfef0-a3db-4dfb-b5e8-5f76011c4a3d",
      "cell_type": "code",
      "source": "# check what an entry looks like, using entries[0] as an example\nentries[0]",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c54164e7-7bd3-4ce0-8387-715d27924831",
      "cell_type": "code",
      "source": "# each entry is a BeatifulSoup structure\ntype(entries[0])",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9daf3d51-a630-4119-98f9-dfc5e601de52",
      "cell_type": "code",
      "source": "#  We can apply several bs4 methods to it.\nentries[0].text",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b4222d6b-0be8-43d5-b05b-383b6d822e65",
      "cell_type": "code",
      "source": "# use the bs4 find_all method to find the 'a' HTML tags for a single entry\n# The \"a\" HTML tags identify links.\nlinks = entries[0].find_all('a', href=True)\n\n# check what we got\nlinks",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "691ddcf9-35c5-4dfe-93bc-cb9f475a3bc9",
      "cell_type": "code",
      "source": "# we got a list\n# retain only the first element\n# because the second refers to the person who posted the job opportunity.\n\n# print out the \"href\", i.e. the actual link of the HTML tag\nfor link in links:\n    print(link['href'])",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5ee87fa4-485d-43d5-b42f-fce701f83608",
      "cell_type": "code",
      "source": "#  the first element:\nprint(links[0]['href'])",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "91212f1d-b170-4339-83cf-8df6b6812663",
      "cell_type": "markdown",
      "source": "### Parse the text and store it in a list\n\nParse the text of each item of the 'entries' list using some Python constructs, as well as a _regular expression_. Look at the text for an item (see above); it is something like:\n\n```\n'\\n\\n\\n\\nOpportunity: Bioinformatics Data Analyst @ Bowie State University -- Bowie, MD (US)\\nSubmitted by Konda Reddy Karnati; posted on Friday,\\xa0January\\xa028,\\xa02022 \\n\\n\\n\\n'\n```\n\n- use strip() to remove the \\n characters at the beginning and at the end of each line\n- the text contained between ':' and '@' is the job title\n- the text contained between '@' and '\\n' is the job location\n- the text after 'posted on ' is the publication data. Here we also need to replace \\xa0 with a regular space\n\nAlso, a link will be something like this (see above):\n\n```\nhttps://www.bioinformatics.org/forums/forum.php?forum_id=14619\n```\n\n- the text after 'forum_id=' is the job number, so  extract that as well\n\nPut all the extracted elements in the list called `my_jobs`.",
      "metadata": {}
    },
    {
      "id": "2f246ed6",
      "cell_type": "code",
      "source": "my_jobs = list()\nfor entry in entries:\n    for l in entry.find_all('a', href=True):\n        # parse the links until we find 'forum' in the URL\n        if 'forum' in l['href']:\n            link = l['href']\n            break\n    else:\n        link = None\n    \n    text = entry.text.strip()\n    \n    # parse the text, grouping the interesting parts as explained above\n    m = re.search('Opportunity: (.+?) @ (.+?)\\n.+?; posted on (.+?)$', text)\n    \n    # extract the various groups of the regular expression\n    title = m.group(1)\n    location = m.group(2)\n    date = m.group(3).replace('\\xa0', ' ') # replace also \\xa0 with a space\n    \n    # extract the job id from the URL\n    if link:\n        m = re.search('.*forum_id=(.+)$', link)\n        job_id = m.group(1)\n    else:\n        job_id = None\n    \n    # finally, append all the extracted elements to a list\n    my_jobs.append([job_id, title, location, link, date])",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ccf4193a-05c6-4f5e-b80b-7f5fa7059b01",
      "cell_type": "code",
      "source": "# check what we got in the end\nmy_jobs",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1a7709e1-fe6a-4b89-b65f-f992e911742f",
      "cell_type": "code",
      "source": "# print it with some formatting\nfor job in my_jobs:\n    print(\"Job number: %s\" % job[0])\n    print(\"\\tTitle: %s\" % job[1])\n    print(\"\\tLocation: %s\" % job[2])\n    print(\"\\tURL: %s\" % job[3])\n    print(\"\\tPublished on: %s\\n\" % job[4])",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aadfba4b-ca37-41a2-922e-9ce64a4d0c68",
      "cell_type": "markdown",
      "source": "### Getting information from a secondary page\n\nApply the same web scraping techniques also to the pages detailing each job. For example, go through the list of jobs that are retrieved so far, visit the respective URLs, and fetch the \"DEADLINE\". Store this deadline into a python dictionary called `my_deadlines`; in this dictionary, define the key to be the job ID, and as value the deadline.\n\nLooking at the HTML for one of the URLs, the deadline is contained in a class called `sf-news`, and the text of that class is \"DEADLINE\". There might be more than one instance of the `sf-news` class, so loop through all of them, and stop when find the one with the DEADLINE text. The actual deadline is a container in the next element, which can be found in the `next_siblings` attribute of the class.\n\nThe deadline string will have some \\r or \\n characters before and after it, so we will remove them with `strip()`.\n\n__However__, the \"DEADLINE\" field is _not_ mandatory, so it might not be present on a page. Therefore consider also this case.",
      "metadata": {}
    },
    {
      "id": "c428ceca-be2e-4de4-9521-55acfc682bff",
      "cell_type": "code",
      "source": "# verify how a job looks like, printing for example my_jobs[0]\nmy_jobs[0]",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "044e39f8-b100-4681-a5dc-7cf3def7fdad",
      "cell_type": "code",
      "source": "my_deadlines = dict()\nfor job in my_jobs:\n    job_id = job[0]\n    job_link = job[3]\n    \n    # get the page referenced by the current job\n    r = requests.get(job_link)\n    \n    # pass the page content to bs4\n    soup = BeautifulSoup(r.content)\n    \n    # find all the \"sf-news\" classes and get the one with text \"DEADLINE\"\n    for c in soup.find_all(class_ = 'sf-news'):\n        if c.text == \"DEADLINE\":\n            # get the deadline, create a dictionary item and then exit from this loop\n            my_deadlines[job_id] = c.next_sibling.strip()\n            break\n    else:\n        # this part gets executed if no \"break\" was encountered.\n        my_deadlines[job_id] = 'No deadline'",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ab8511ff-544b-4383-8e2f-81e0c9291362",
      "cell_type": "code",
      "source": "# check that my_deadlines contains what we want\nmy_deadlines",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "19484403-5c1d-48cd-a2d6-8b2f0cd93c91",
      "cell_type": "markdown",
      "source": "### Combine everything into a single block of code and store the info in a database\n\nCombine everything into a single block of code, and for example print out all the gathered informations\n",
      "metadata": {}
    },
    {
      "id": "08a2aff4-17da-4417-bd4b-5a0e061b7455",
      "cell_type": "code",
      "source": "# this would be the final code, from start to end\n\nLENGTH = 10 # number of jobs to retrieve\nJOB_URL = 'https://www.bioinformatics.org/jobs/?group_id=101&summaries=1&length=%s' % (LENGTH-1)\n\n# get the HTML of the page with requests.get()\nr = requests.get(JOB_URL)\n\n# pass the page content to bs4\nsoup = BeautifulSoup(r.content)\n\n# get all the tables\ntables = soup.find_all('table')\n\n# store the interesting tables in a list\nentries = list()\nfor table in tables:\n    if 'Opportunity' in table.text:\n        entries.append(table)\n\n# remove the first two entries\nentries = entries[2:]\n\nfor entry in entries:\n    for l in entry.find_all('a', href=True):\n        # parse the links until we find 'forum' in the URL\n        if 'forum' in l['href']:\n            link = l['href']\n            break\n    else:\n        link = None\n\n    text = entry.text.strip()\n    \n    # parse the text, grouping the interesting parts \n    m = re.search('Opportunity: (.+?) @ (.+?)\\n.+?; posted on (.+?)$', text)\n    \n    # extract the various groups of the regular expression\n    title = m.group(1)\n    location = m.group(2)\n    date = m.group(3).replace('\\xa0', ' ') # replace also \\xa0 with a space\n    \n    # extract the job id from the link\n    if link:\n        m = re.search('.*forum_id=(.+)$', link)\n        job_id = m.group(1)\n        # get the deadline by scraping the link\n        link_page = requests.get(link)\n        # pass the page content to bs4\n        link_soup = BeautifulSoup(link_page.content)\n        \n        # find all the \"sf-news\" classes and get the one with text \"DEADLINE\"\n        for c in link_soup.find_all(class_ = 'sf-news'):\n            if c.text == \"DEADLINE\":\n                # get the deadline text. remove spurious characters and exit from this loop\n                deadline = c.next_sibling.strip()\n                break\n        else:\n            # this part gets executed if no \"break\" was encountered.\n            deadline = 'No deadline'\n    else:\n        job_id = None\n    \n    # print out all that we have gathered, with some formatting\n    print(\"Job number: %s\" % job_id)\n    print(\"\\tTitle: %s\" % title)\n    print(\"\\tLocation: %s\" % location)\n    print(\"\\tURL: %s\" % link)\n    print(\"\\tPublished on: %s\" % date)\n    if link:\n        print(\"\\tDeadline: %s\" % deadline)\n    else:\n        print(\"\\tDeadline: unknown\")\n    ",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}